{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from gradio import processing_utils\n",
    "import json\n",
    "from transformers import LlamaTokenizer, WhisperFeatureExtractor\n",
    "from transformers import GenerationConfig\n",
    "from blsp.src.modeling_blsp import BlspModel\n",
    "from blsp.src.speech_text_paired_dataset import get_waveform\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Speech-Language Demo\")\n",
    "    parser.add_argument(\n",
    "        \"--blsp_model\", type=str, default='checkpoints/stage2',\n",
    "        help=\"Path to the blsp model\"\n",
    "    )\n",
    "    ### args for generation\n",
    "    parser.add_argument(\n",
    "        \"--max_new_tokens\", type=int, default=128,\n",
    "        help=\"max new tokens for generation\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_new_tokens\", type=int, default=1,\n",
    "        help=\"min new tokens for generation\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--temperature\", type=float, default=0.1,\n",
    "        help=\"temperature for generation\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--top_p\", type=float, default=0.75,\n",
    "        help=\"top_p for generation\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=128,\n",
    "    min_new_tokens=1,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.75,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "class ChatHistory(object):\n",
    "    def __init__(self, tokenizer, extractor):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.extractor = extractor\n",
    "        self.history = []\n",
    "        self.audio_file = []\n",
    "        self.audio_to_history = True\n",
    "\n",
    "        ### add bos token\n",
    "        self.add_bos()\n",
    "\n",
    "    def add_bos(self):\n",
    "        input_ids = tokenizer(\"\", return_tensors=\"pt\").input_ids.cuda()\n",
    "        self.history.append(\n",
    "            (input_ids,)\n",
    "        )\n",
    "\n",
    "    def add_text_history(self, text):\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids[:,1:].cuda()\n",
    "        self.history.append(\n",
    "            (input_ids,)\n",
    "        )\n",
    "\n",
    "    def add_audio(self, audio_file):\n",
    "        self.audio_to_history = False\n",
    "        self.audio_file.append(audio_file)\n",
    "\n",
    "    def add_speech_history(self, speech):\n",
    "        if self.audio_to_history:\n",
    "            return\n",
    "        self.audio_to_history = True\n",
    "        speech = get_waveform(speech, output_sample_rate=self.extractor.sampling_rate)\n",
    "        speech_inputs = self.extractor(\n",
    "            speech,\n",
    "            sampling_rate=self.extractor.sampling_rate,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        speech_values = speech_inputs.input_features.cuda()\n",
    "        speech_attention_mask = speech_inputs.attention_mask.cuda()\n",
    "        self.history.append(\n",
    "            (speech_values, speech_attention_mask)\n",
    "        )\n",
    "\n",
    "print('Initializing Chat')\n",
    "args = parse_args()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(args.blsp_model)\n",
    "extractor = WhisperFeatureExtractor.from_pretrained(args.blsp_model)\n",
    "model = BlspModel.from_pretrained(args.blsp_model)\n",
    "\n",
    "generation_config.update(\n",
    "    **{\n",
    "        \"max_new_tokens\": args.max_new_tokens,\n",
    "        \"min_new_tokens\": args.min_new_tokens,\n",
    "        \"temperature\": args.temperature,\n",
    "        \"top_p\": args.top_p,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"bos_token_id\": tokenizer.bos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    ")\n",
    "\n",
    "def add_text(user_message):\n",
    "    history.add_text_history(\"###[Human]:\")\n",
    "    history.add_text_history(user_message)\n",
    "    history.add_text_history(\"\\n\\n\\n###[Assistant]:\")\n",
    "    \n",
    "def add_file(audio_file):\n",
    "    history.add_text_history(\"###[Human]:\")\n",
    "    history.add_audio(audio_file)\n",
    "    history.add_speech_history(history.audio_file[-1])\n",
    "    history.add_text_history(\"\\n\\n\\n###[Assistant]:\")\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
